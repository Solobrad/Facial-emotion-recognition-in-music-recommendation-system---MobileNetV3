{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import gc\n",
    "import h5py\n",
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "from wandb.integration.keras import WandbCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Conv2D, Dropout, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.applications import MobileNetV2, MobileNetV3Small, MobileNetV3Large\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "load_dotenv()  # Load environment variables from .env file\n",
    "api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "\n",
    "wandb.login(key=api_key)  # Login with the API key\n",
    "wandb_project = \"FER_Model\"\n",
    "wandb_username = os.getenv(\"WANDB_USERNAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(hdf5_file, dataset_name_images, dataset_name_labels):\n",
    "    with h5py.File(hdf5_file, 'r') as hf:\n",
    "        data = np.array(hf[dataset_name_images])\n",
    "        labels = np.array(hf[dataset_name_labels])\n",
    "    return data, labels\n",
    "\n",
    "# Load data\n",
    "data, labels = load_data('output files/fer2013_processed.h5', 'fer2013_images', 'fer2013_labels')\n",
    "\n",
    "# Print shapes to verify\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "\n",
    "# Ensure that data and labels are not empty\n",
    "assert len(data) > 0, \"Data is empty.\"\n",
    "assert len(labels) > 0, \"Labels are empty.\"\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    data, \n",
    "    labels, \n",
    "    test_size=0.2, \n",
    "    stratify=labels\n",
    ")\n",
    "\n",
    "# Print shapes of the split data\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "print(f\"Validation data shape: {X_val.shape}\")\n",
    "print(f\"Validation labels shape: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define the data augmentation transformations\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=20,         # Randomly rotate images by up to 20 degrees\n",
    "    width_shift_range=0.2,     # Randomly shift images horizontally by up to 20% of the width\n",
    "    height_shift_range=0.2,    # Randomly shift images vertically by up to 20% of the height\n",
    "    shear_range=0.2,           # Randomly apply shearing transformation\n",
    "    zoom_range=0.2,            # Randomly zoom images by up to 20%\n",
    "    horizontal_flip=True,      # Randomly flip images horizontally\n",
    "    fill_mode='nearest'        # Strategy for filling in newly created pixels\n",
    ")\n",
    "\n",
    "# Create data generator for training data\n",
    "train_generator = train_datagen.flow(\n",
    "    X_train,   # Training images\n",
    "    y_train,   # Training labels\n",
    "    batch_size=64  # Batch size for training\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator()\n",
    "\n",
    "# Create data generator for validation data\n",
    "validation_generator = validation_datagen.flow(\n",
    "    X_val,  # Validation images\n",
    "    y_val,  # Validation labels\n",
    "    batch_size=64   # Batch size for validation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',  # Metric to monitor, e.g., validation loss\n",
    "    factor=0.5,          # Factor by which the learning rate will be reduced\n",
    "    patience=5,          # Number of epochs with no improvement before reducing LR\n",
    "    min_lr=1e-7,         # Lower bound for the learning rate\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
